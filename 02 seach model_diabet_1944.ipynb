{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Внимание!!! Важно, что бы файлы с данными и исполняемый файл находились в одной папке, \n",
    "# тогда пути к тестовым и тренировочным наборам будут содержать только имена файлов.\n",
    "# \n",
    "# В пути к тренировочным и тестовым данным запрежается использовать абсалютную адресацию, \n",
    "# то есть адресацию, в которой присутствуют имена папок. Путь должен содержать только имя файла.\n",
    "#\n",
    "# Напоминание: под моделью машинного обучения понимаются все действия с исходными данными, \n",
    "# которые необходимо произвести, что бы сопоставить признаки целевому значению."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Область работы 1 (библиотеки)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Данный блок в области 1 НЕ выполняется преподавателем\n",
    "# \n",
    "# данный блок предназначен только для подключения необходимых библиотек\n",
    "# запрещается подключать библиотеки в других блоках\n",
    "#\n",
    "# установка дополнительных библиотек размещается прямо здесь (обязательно закоментированы)\n",
    "#\n",
    "# pip install\n",
    "# pip install catboost\n",
    "# pip install lightgbm\n",
    "# pip install xgboost\n",
    "# pip install shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_predict\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import catboost as cb\n",
    "import shap\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Область работы 2 (поиск  модели .... )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train_diab.csv')\n",
    "test = pd.read_csv('test_diab.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['ethnicity', 'education_level', 'employment_status']\n",
    "\n",
    "# Удаляем из тренировочных данных\n",
    "for col in columns_to_drop:\n",
    "    if col in train.columns:\n",
    "        train = train.drop(col, axis=1)\n",
    "\n",
    "# Удаляем из тестовых данных\n",
    "for col in columns_to_drop:\n",
    "    if col in test.columns:\n",
    "        test = test.drop(col, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Feature Engineering функция\n",
    "def create_features(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Медицинские соотношения\n",
    "    df['bp_ratio'] = df['systolic_bp'] / (df['diastolic_bp'] + 1e-5)\n",
    "    df['cholesterol_ratio'] = df['cholesterol_total'] / (df['hdl_cholesterol'] + 1e-5)\n",
    "    df['ldl_hdl_ratio'] = df['ldl_cholesterol'] / (df['hdl_cholesterol'] + 1e-5)\n",
    "    df['triglycerides_hdl_ratio'] = df['triglycerides'] / (df['hdl_cholesterol'] + 1e-5)\n",
    "    \n",
    "    # Категоризация на основе медицинских норм\n",
    "    df['bmi_category'] = pd.cut(df['bmi'], \n",
    "                                 bins=[0, 18.5, 25, 30, 35, 40, 100], \n",
    "                                 labels=[0, 1, 2, 3, 4, 5])\n",
    "    \n",
    "    df['bp_category'] = np.where(\n",
    "        (df['systolic_bp'] >= 140) | (df['diastolic_bp'] >= 90), 2,\n",
    "        np.where((df['systolic_bp'] >= 130) | (df['diastolic_bp'] >= 85), 1, 0)\n",
    "    )\n",
    "    \n",
    "    df['age_group'] = pd.cut(df['age'], \n",
    "                              bins=[0, 30, 40, 50, 60, 70, 100], \n",
    "                              labels=[0, 1, 2, 3, 4, 5])\n",
    "    \n",
    "    df['activity_level'] = pd.cut(df['physical_activity_minutes_per_week'],\n",
    "                                   bins=[0, 30, 90, 150, 300, 1000],\n",
    "                                   labels=[0, 1, 2, 3, 4])\n",
    "    \n",
    "    # Полиномиальные и взаимодействующие признаки\n",
    "    df['bmi_age'] = df['bmi'] * df['age'] / 100\n",
    "    df['bmi_cholesterol'] = df['bmi'] * df['cholesterol_total'] / 1000\n",
    "    df['age_cholesterol'] = df['age'] * df['cholesterol_total'] / 1000\n",
    "    df['waist_bmi'] = df['waist_to_hip_ratio'] * df['bmi']\n",
    "    \n",
    "    # Статистические признаки\n",
    "    df['total_risk_score'] = (\n",
    "        (df['bmi'] > 30).astype(int) +\n",
    "        (df['age'] > 50).astype(int) +\n",
    "        (df['systolic_bp'] > 140).astype(int) +\n",
    "        (df['family_history_diabetes'] > 0).astype(int) +\n",
    "        (df['physical_activity_minutes_per_week'] < 150).astype(int)\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Применяем feature engineering\n",
    "train = create_features(train)\n",
    "test = create_features(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Разделение признаков\n",
    "TARGET = 'diagnosed_diabetes'\n",
    "ID_COL = 'id'\n",
    "\n",
    "# Удаляем ID и целевую переменную из признаков\n",
    "X = train.drop([ID_COL, TARGET], axis=1)\n",
    "y = train[TARGET]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Определяем типы признаков\n",
    "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_features = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "# Категориальные признаки\n",
    "categorical_features = [col for col in ['gender', 'income_level', 'smoking_status',\n",
    "                                        'bmi_category', 'age_group', 'bp_category', \n",
    "                                        'activity_level']\n",
    "                        if col in X.columns]\n",
    "\n",
    "numeric_features = [col for col in numeric_features if col not in categorical_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Создание pipeline с кросс-валидационным подходом\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "\n",
    "# Предобработка для числовых признаков\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Предобработка для категориальных признаков\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "# Объединение трансформеров\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Feature selection с использованием SHAP\n",
    "print(\"Начинаем feature selection с SHAP...\")\n",
    "\n",
    "# Сначала обучим модель на всех признаках для оценки важности\n",
    "lgb_base = lgb.LGBMClassifier(\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=5,\n",
    "    n_jobs=-1,\n",
    "    verbosity=-1\n",
    ")\n",
    "\n",
    "# Создаем временный pipeline для оценки важности признаков\n",
    "temp_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', lgb_base)\n",
    "])\n",
    "\n",
    "# Обучаем на одном фолде для скорости\n",
    "train_idx, val_idx = next(skf.split(X, y))\n",
    "X_temp, y_temp = X.iloc[train_idx], y.iloc[train_idx]\n",
    "temp_pipeline.fit(X_temp, y_temp)\n",
    "\n",
    "# Получаем важность признаков через SHAP\n",
    "explainer = shap.TreeExplainer(temp_pipeline.named_steps['classifier'])\n",
    "transformed_features = temp_pipeline.named_steps['preprocessor'].transform(X.iloc[X_temp])\n",
    "shap_values = explainer.shap_values(transformed_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Если бинарная классификация, shap_values будет списком [для класса 0, для класса 1]\n",
    "if isinstance(shap_values, list):\n",
    "    shap_values = np.abs(shap_values[1])  # берем значения для положительного класса\n",
    "else:\n",
    "    shap_values = np.abs(shap_values)\n",
    "\n",
    "mean_shap_values = np.mean(shap_values, axis=0)\n",
    "feature_names = numeric_features + list(temp_pipeline.named_steps['preprocessor']\n",
    "                                       .named_transformers_['cat']\n",
    "                                       .named_steps['onehot']\n",
    "                                       .get_feature_names_out(categorical_features))\n",
    "\n",
    "# Выбираем топ-N признаков по SHAP значениям\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'shap_importance': mean_shap_values\n",
    "}).sort_values('shap_importance', ascending=False)\n",
    "\n",
    "top_features = feature_importance_df.head(50)['feature'].tolist()  # берем топ-50 признаков\n",
    "print(f\"Выбрано {len(top_features)} наиболее важных признаков\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Создание финального pipeline с отобранными признаками\n",
    "# Модели для стекинга\n",
    "base_models = [\n",
    "    ('lgbm', lgb.LGBMClassifier(\n",
    "        n_estimators=1000,\n",
    "        learning_rate=0.01,\n",
    "        max_depth=6,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        reg_alpha=0.1,\n",
    "        reg_lambda=0.1,\n",
    "        n_jobs=-1,\n",
    "        verbosity=-1\n",
    "    )),\n",
    "    ('xgb', xgb.XGBClassifier(\n",
    "        n_estimators=1000,\n",
    "        learning_rate=0.01,\n",
    "        max_depth=6,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        reg_alpha=0.1,\n",
    "        reg_lambda=0.1,\n",
    "        n_jobs=-1,\n",
    "        eval_metric='auc',\n",
    "        use_label_encoder=False\n",
    "    )),\n",
    "    ('catboost', cb.CatBoostClassifier(\n",
    "        iterations=1000,\n",
    "        learning_rate=0.01,\n",
    "        depth=6,\n",
    "        l2_leaf_reg=3,\n",
    "        verbose=False,\n",
    "        thread_count=-1\n",
    "    )),\n",
    "    ('rf', RandomForestClassifier(\n",
    "        n_estimators=500,\n",
    "        max_depth=10,\n",
    "        min_samples_split=5,\n",
    "        min_samples_leaf=2,\n",
    "        max_features='sqrt',\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "]\n",
    "\n",
    "# Мета-модель\n",
    "meta_model = LogisticRegression(\n",
    "    C=0.1,\n",
    "    max_iter=1000,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Создаем стекинг классификатор\n",
    "stacking_model = StackingClassifier(\n",
    "    estimators=base_models,\n",
    "    final_estimator=meta_model,\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    "    passthrough=True  # Используем исходные признаки + предсказания базовых моделей\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Калиброванный классификатор\n",
    "calibrated_model = CalibratedClassifierCV(\n",
    "    stacking_model,\n",
    "    method='isotonic',\n",
    "    cv=3\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Финальный pipeline\n",
    "final_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('feature_selector', SelectFromModel(\n",
    "        RandomForestClassifier(n_estimators=100),\n",
    "        threshold='median'\n",
    "    )),\n",
    "    ('classifier', calibrated_model)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Кросс-валидация с out-of-fold predictions\n",
    "print(\"\\nНачинаем кросс-валидацию...\")\n",
    "oof_predictions = np.zeros(len(X))\n",
    "feature_importance_list = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X, y), 1):\n",
    "    print(f\"\\nФолд {fold}/5:\")\n",
    "    \n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "    \n",
    "    # Обучаем pipeline\n",
    "    final_pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Предсказания на валидации\n",
    "    val_preds = final_pipeline.predict_proba(X_val)[:, 1]\n",
    "    oof_predictions[val_idx] = val_preds\n",
    "    \n",
    "    # Оценка на фолде\n",
    "    fold_auc = roc_auc_score(y_val, val_preds)\n",
    "    print(f\"AUC на фолде {fold}: {fold_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. Финальная оценка\n",
    "final_auc = roc_auc_score(y, oof_predictions)\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"Финальный OOF AUC: {final_auc:.4f}\")\n",
    "print(f\"{'='*50}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. Обучение на всех данных для финальной модели\n",
    "print(\"\\nОбучаем финальную модель на всех данных...\")\n",
    "final_pipeline.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13. Создание предсказаний для тестового набора\n",
    "print(\"\\nСоздание предсказаний для тестового набора...\")\n",
    "\n",
    "# Подготовка тестовых данных\n",
    "X_test = test.drop([ID_COL], axis=1, errors='ignore')\n",
    "\n",
    "# Проверяем, что все колонки из X есть в X_test\n",
    "missing_cols = set(X.columns) - set(X_test.columns)\n",
    "if missing_cols:\n",
    "    print(f\"Внимание: в тестовых данных отсутствуют колонки: {missing_cols}\")\n",
    "    # Добавляем недостающие колонки с нулевыми значениями\n",
    "    for col in missing_cols:\n",
    "        X_test[col] = 0\n",
    "    # Упорядочиваем колонки в том же порядке, что и в X\n",
    "    X_test = X_test[X.columns]\n",
    "\n",
    "# Делаем предсказания\n",
    "test_predictions = final_pipeline.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14. Создание CSV файла с результатами\n",
    "print(\"\\nСоздание файла с результатами...\")\n",
    "\n",
    "# Создаем DataFrame с результатами\n",
    "submission_df = pd.DataFrame({\n",
    "    'id': test[ID_COL],\n",
    "    'diagnosed_diabetes': test_predictions\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохраняем в CSV файл\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_filename = f'submission_{timestamp}.csv'\n",
    "submission_df.to_csv(output_filename, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
